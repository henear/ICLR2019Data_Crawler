[
{
"Title": "On The Convergence Of Adam And Beyond",
"Keyword": "['Optimization', 'DeepLearning', 'Adam', 'Rmsprop']",
"Rating1": 8,
"Review1": "This paper examines the very popular and useful ADAM optimization algorithm, and locates a mistake in its proof of convergence (for convex problems). Not only that, the authors also show a specific toy convex problem on which ADAM fails to converge. Once the problem was identified to be the decrease in v_t (and increase in learning rate), they modified the algorithm to solve that problem. They then show the modified algorithm does indeed converge and show some experimental results comparing it to ADAM.    The paper is well written, interesting  and very important given the popularity of ADAM.     Remarks:  - The fact that your algorithm cannot increase the learning rate seems like a possible problem in practice. A large gradient at the first steps due to bad initialization can slow the rest of training. The experimental part is limited, as you state *preliminary*, which is a unfortunate for a work with possibly an important practical implication. Considering how easy it is to run experiments with standard networks using open-source software, this can easily improve the paper. That being said, I understand that the focus of this work is theoretical and well deserves to be accepted based on the theoretical work.    - On page 14 the fourth inequality not is clear to me.    - On page 6 you talk about an alternative algorithm using smoothed gradients which you do not mention anywhere else and this isn't that clear (more then one way to smooth). A simple pseudo-code in the appendix would be welcome.    Minor remarks:  - After the proof of theorem 1 you jump to the proof of theorem 6 (which isn't in the paper) and then continue with theorem 2. It is a bit confusing.  - Page 16 at the bottom v_t= ... sum beta^{t-1-i}g_i should be g_i^2  - Page 19 second line, you switch between j&t and it is confusing. Better notation would help.  - The cifarnet uses LRN layer that isn't used anymore.",
"Comment11": "We thank the reviewer for the helpful and supportive feedback. The focus of the paper is to provide a principled understanding for the exponential moving average (EMA) adaptive optimization methods, which are now used as building blocks of many modern deep learning applications. The counter-example for non-convergence we show is very natural and is observed to arise in extremely sparse real-world problems (e.g., pertaining to problems with large output spaces). We provided two general directions to address the convergence issues in these algorithms (by either changing the structure of the algorithm or by gradually increasing beta2 as algorithm proceeds). We have provided preliminary experiments on a few commonly used networks & datasets but we do agree that a thorough empirical study will be very useful and is part of our future plan.     - Fourth inequality on Page 14: We revised the paper to explain it further.  - We will be happy to elaborate our comment about smoothed gradients in the final version of the paper.  - We also addressed other minor suggestions.",
"Comment12": "Dear authors,    It's a very good paper, but I have some questions as follows:    (1) In the last paragraph on Page 14, it says the fourth inequality is from $\\beta^{i'-1}_2 C^2 \\le 1$, but I couldn't go through from the third inequality to the fourth inequality on Page 14. It seems that you applied the lower bound of $v_{t+i-1}$ (i.e. $v_{t+i-1} \\ge (1-\\beta)\\beta^{i-1}_2 C^2$ which is not desired) instead of its upper bound (which is truly required)?     (2) In Corollary 1, from my understanding, the L2 norm of $g_{1:T,1}$ should be upper bounded by $\\sqrt(T)G_{\\inf}$, so the regret be $O(\\sqrt(T \\logT))$ instead of $O(\\sqrt(T))$ as stated in the remark of Corollary 1.     Correct me if I'm wrong. Thanks!",
"Comment13": "(1) Thanks for the interest in our paper and looking into the analysis carefully. We believe there is a misunderstanding regarding the proof. The third inequality follows from the lower bound v_{t+i-1} \\ge (1-\\beta)\\beta^{i-1}_2 C^2. The fourth inequality actually follows from the upper bound on v_{t+i-1}  (which implicitly uses \\beta^{i'-1}_2 C^2 \\le 1). We revised the paper to provide the detailed derivation, including specifying precise constants that were previously omitted.    (2) Actually, an easy observation from our analysis is that we can bound the regret of AMSGrad by O(G_infty sqrt(T)) as well. This can be easily seen from the proof of Lemma 2 where in the analysis the term \\sum_{t=1}^T |g_{t,i}/\\sqrt{t} can be bounded by O(G_infty sqrt(T)) instead of O(\\sqrt{\\log(T) ||g_{1:T}||_2). Thus, the regret of AMSGrad is upper bounded by minimum of O(G_infty sqrt(T)) and the bound presented in Theorem 4, and thus the worst case dependence on T is \\sqrt{T} rather than \\sqrt{T \\log(T)}. We will make this point in the final version of the paper.",
"Comment14": "Thanks for the inspiring paper. The observations are interesting and important!    It is easy to capture that exponential moving average might not able to capture the long-term memory of the gradients.     The paper is mainly focused on the beta2 that involving the averaging of second moment. It makes me wonder whether the beta1 on the averaging of the first moment gradient also suffer the similar problem.     It seems a direct solution would be using a large beta1 and beta2. (Always keep the maximum of the entire history seems is not the best solution and an average over a recent history might be a better alternative.)     I did not carefully check the detail of the paper. But generally, one would have a similar concern I think. Could you explain the benefits of the proposed algorithm?     The synthetic experiments seem to use a relatively insufficient large beta2 regarding the large gradient gap, which makes it not able to capture the necessary long-term dependency. ",
"Comment15": "Thanks for your interest in our paper and for your feedback. We believe that beta1 is not an issue for convergence of Adam (although our theoretical analysis assumes a decreasing beta1). For example, in stochastic convex optimization, momentum based methods have been shown to converge even for constant beta1. That said, it is indeed interesting to develop better understanding of the effect of momentum in convergence of these algorithms (especially in the nonconvex setting).    As the paper shows, for any constant beta2, there exists a counter-example for non-convergence of Adam (both in online as well as stochastic setting, Theorem 2 & Theorem 3). Using a large beta2 can partially mitigate this issue in practice but it is not clear how high beta2 should be and it is indeed an interesting research question. Our paper proposes a couple of approaches (AMSGrad & AdamNC) for addressing these issues. AMSGrad allows us to use a fixed beta2 by changing the structure of the algorithm (and also allows us to use a much slow decaying learning rate than Adagrad). AdamNC looks at an approach where beta2 changes with t, ultimately converging to 1, hopefully allowing us to retain the benefits of Adam but at the same time circumventing its non-convergence.    The aim of synthetic experiments was to demonstrate the effect of non-convergence. We can modify it to demonstrate similar problem for any constant beta2.",
"Comment16": "Congratulations for this paper, I really enjoyed it. It is a well written paper that contains an exhaustive set of counterexamples. I had also noticed that the proof of Adam was wrong and included it in my Master Thesis  (https://damaru2.github.io/convergence_analysis_hypergradient_descent/dissertation_hypergradients.pdf Section 2.4) and I enjoyed reading through the paper and finding that indeed it was not just that the proof was wrong but that the method does not converge in general, not even in the stochastic case.    I noticed some typos / minor things that seem that need to be fixed:    + In the penultimate line of page 16 there is this equality v_{t-1} = .... g_i. This g_i should be squared.    + In the following line, there is another square missing in a C, it should be (1-\\beta_{t-1}_2)(C^2 p + (1-p)) and there is a pair of parenthesis missing in the next term, it should be  (1-\\beta_2^{t-1})((1+\\delta)C-\\delta)    + The fact that in Theorems 2 and 3 \\beta_2 is allowed to be 1 is confusing, since the method is not well defined if \\beta_2 is 1 (and you don't use an \\epsilon in the denominator. If you use an \\epsilon then with \\beta_1 = 0 the method is equivalent to SGD so it converges for a choice of alpha). In particular, in the proof of theorem 3 \\sqrt{1-\\beta_2}  appears in some denominators and so does \\sqrt{\\beta_2} but there is no comment about what happens when this quantities are 0. There should be a quick comment on this or the \\beta_2 \\leq 1 should be removed from the theorems.    Best wishes",
"Comment17": "Thanks David, for your interest in this paper and helpful comments (and pointers). We have addressed your concerns regarding typos in the latest revision of the paper.",
"Rating2": 8,
"Review2": "This work identifies a mistake in the existing proof of convergence of  Adam, which is among the most popular optimization methods in deep  learning. Moreover, it gives a simple 1-dimensional counterexample with  linear losses on which Adam does not converge. The same issue also  affects RMSprop, which may be viewed as a special case of Adam without  momentum. The problem with Adam is that the *learning rate* matrices  V_t^{1/2}/alpha_t are not monotonically decreasing. A new method, called  AMSGrad is therefore proposed, which modifies Adam by forcing these  matrices to be decreasing. It is then shown that AMSGrad does satisfy  essentially the same convergence bound as the one previously claimed for  Adam. Experiments and simulations are provided that support the  theoretical analysis.    Apart from some issues with the technical presentation (see below), the  paper is well-written.    Given the popularity of Adam, I consider this paper to make a very  interesting observation. I further believe all issues with the technical  presentation can be readily addressed.        Issues with Technical Presentation:    - All theorems should explicitly state the conditions they require    instead of referring to *all the conditions in (Kingma & Ba, 2015)*.  - Theorem 2 is a repetition of Theorem 1 (except for additional    conditions).  - The proof of Theorem 3 assumes there are no projections, so this    should be stated as part of its conditions. (The claim in footnote 2    that they can be handled seems highly plausible, but you should be up    front about the limitations of your results.)  - The regret bound Theorem 4 establishes convergence of the optimization    method, so it plays the role of a sanity check. However, it is    strictly worse than the regret bound O(sqrt{T}) for online gradient    descent [Zinkevich,2003], so it cannot explain why the proposed    AMSgrad method might be adaptive. (The method may indeed be adaptive    in some sense; I am just saying the *bound* does not express that.    This is also not a criticism of the current paper; the same remark    also applies to the previously claimed regret bound for Adam.)  - The discussion following Corollary 1 suggests that sum_i    hat{v}_{T,i}^{1/2} might be much smaller than d G_infty. This is true,    but we should always expect it to be at least a constant, because    hat{v}_{t,i} is monotonically increasing by definition of the    algorithm, so the bound does not get better than O(sqrt(T)).    It is also suggested that sum_i ||g_{1:T,i}|| = sqrt{sum_{t=1}^T    g_{t,i}^2} might be much smaller than dG_infty, but this is very    unlikely, because this term will typically grow like O(sqrt{T}),    unless the data are extremely sparse, so we should at least expect    some dependence on T.  - In the proof of Theorem 1, the initial point is taken to be x_1 = 1,    which is perfectly fine, but it is not *without loss of generality*,    as claimed. This should be stated in the statement of the Theorem.  - The proof of Theorem 6 in appendix B only covers epsilon=1. If it is    *easy to show* that the same construction also works for other    epsilon, as claimed, then please provide the proof for general    epsilon.      Other remarks:    - Theoretically, nonconvergence of Adam seems a severe problem. Can you    speculate on why this issue has not prevented its widespread adoption?    Which factors might mitigate the issue in practice?  - Please define g_t \\circ g_t and g_{1:T,i}  - I would recommend sticking with standard linear algebra notation for    the sqrt and the inverse of a matrix and simply using A^{-1} and    A^{1/2} instead of 1/A and sqrt{A}.  - In theorems 1,2,3, I would recommend stating the dimension (d=1) of    your counterexamples, which makes them very nice!    Minor issues:    - Check accent on Nicol\\`o Cesa-Bianchi in bibliography.  - Near the end of the proof of Theorem 6: I believe you mean Adam    suffers a *regret* instead of a *loss* of at least 2C-4.    Also 2C-4=2C-4 is trivial in the second but last display.",
"Comment21": "We deeply appreciate the reviewer for a thorough and constructive feedback.     - Theorem 2 & 3 are much more involved and hence the aim of Theorem 1 was to provide a simplified counter-example for a restrictive setting, thereby providing the key ideas of the paper.  - We will emphasize your point about projections in the final version of the paper.  - We agree that the role of Theorem 4 right now is to provide a sanity check. Indeed, it is not possible to improve upon the of online gradient descent in the worst case convex settings. Algorithms such as Adagrad exploit structure in the problem such as sparsity to provide improved regret bounds. Theorem 4 provides some adaptivity to sparsity of gradients (but note that these are upper bounds and it is not clear if they are tight). Adaptive methods seem to perform well in few non-sparse and nonconvex settings too. It remains open to understand it in the nonconvex settings of our interest.   - Indeed, there is a typo; we expect ||g{1:T,i}|| to grow like sqrt(T). The main benefit in adaptive methods comes in terms of sparsity (and dimension dependence). For example see Section 1.3 in Duchi et al. 2011). We have revised the paper to incorporate these changes.  - We can indeed assume that x_1 = 1 (without loss of generality) because for any choice of initial point, we can always translate the function so that x_1 = 1 is the initial point in the new coordinate system. We will add a discussion about this in the final version of the paper.  - The last part of Theorem 6 explains the reduction with respect to general epsilon. We will further highlight this in the final version of the paper.    Other remarks:    Regarding widespread adoption of Adam: It is possible that in certain applications the issues we raised in this work are not that severe (although they can still lead to degradation in generalization performance). On the contrary, there exist a large number of real-world applications, for instance training models with large output spaces, which suffer from the issues we have highlighted and non-convergence has been observed to occur more frequently. Often, this non-convergence is attributed to nonconvexity but our paper shows one of the causes that applies even to convex settings.   As stated in the paper, using a problem specific large beta2 seems to help in some applications. Researchers have developed many tricks (such as gradient clipping) which might also play a role in mitigating these issues. We propose two different approaches to fix this issue and it will be interesting to investigate these approaches in various applications.    We have addressed all other minor concerns directly in the revision of the paper.",
"Rating3": 9,
"Review3": "The paper presents three contributions: 1) it shows that the proof of convergence Adam is wrong; 2) it presents adversarial and stochastic examples on which Adam converges to the worst possible solution (i.e. there is no hope to just fix Adam's proof); 3) it proposes a variant of Adam called AMSGrad that fixes the problems in the original proof and seems to have good empirical properties.    The contribution of this paper is very relevant to ICLR and, as far as I know, novel.  The result is clearly very important for the deep learning community.  I also checked most of the proofs and they look correct to me: The arguments are quite standard, even if the proofs are very long.    One note on the generality of the results: the papers states that some of the results could apply to RMSProp too. However, it has been proved that RMSProp with a certain settings of its parameters is nothing else than AdaGrad (see Section 4 in  Mukkamala and Hein, ICML'17). Hence, at least for a certain setting of its parameters, RMSProp will converge. Of course, the proof in the ICML paper could be wrong, I did not check that...    A general note on the learning rate: The fact that most of these algorithms are used with a fixed learning rate while the analysis assume a decaying learning rate should hint to the fact that we are not using the right analysis. Indeed, all these variants of AdaGrad did not really improve the AdaGrad's regret bound. In this view, none of these algorithms contributed in any meaningful way to our understanding of the optimization of deep networks *nor* they advanced in any way the state-of-the-art for optimizing convex Lipschitz functions.  On the other hand, analysis of SGD-like algorithms with constant step sizes are known. See, for example, Zhang, ICML'04 where linear convergence is proved in a neighbourhood of the optimal solution for strongly convex problems.  So, even if I understand this is not the main objective of this paper, it would be nice to see a discussion on this point and the limitations of regret analysis to analyse SGD algorithms.    Overall, I strongly suggest to accept this paper.      Suggestions/minor things:  - To facilitate the reader, I would state from the beginning what are the common settings of beta_1 and beta_2 in Adam. This makes easier to see that, for example, the condition of Theorem 2 is verified.  - \\hat{v}_{0} is undefined in Algorithm 2.  - The graphs in figure 2 would gain in readability if the setting of each one of them would be added as their titles.  - McMahan and Streeter (2010) is missing the title. (Also, kudos for citing both the independent works on AdaGrad)  - page 11, last equation, 2C-4=2C-4. Same on page 13.  - Lemma 4 contains x_1,x_2,z_1, and z_2: are x_1 and z_1 the same? also x_2 and z_2?",
"Comment31": "We thank the reviewer for very helpful and constructive feedback.     About Mukkamala and Hein 2017 [MH17]: Thanks for pointing this paper. As the anonymous reviewer rightly points out, the [MH17] does not look at the standard version of RMSProp but rather a modification and thus, there is no contradiction with our paper. We will make this point clear in the final version of the paper.    Regarding note about learning rate: While it is true that none of these new rates improve upon Adagrad rates, in fact, in the worst case one cannot improve the regret of standard online gradient descent in general convex setting. Adagrad improves this in the special case of sparse gradients (see for instance, Section 1.3 of Duchi et al. 2011). However, these algorithms, which are designed for specific convex settings, appear to perform reasonably well in the nonconvex settings too (especially in deep networks). Exponential moving average (EMA) variants seem to further improve the performance in the (dense) nonconvex setting. Understanding the cause for good performance in nonconvex settings is an interesting open problem. Our aim was to take an initial step to develop more principled EMA approaches. We will add a description in the final version of the paper.    Lemma 4: Thanks for pointing it out and sorry for the confusion. Indeed, x1 = z1 and x2 = z2. We have corrected this typo.    We have also revised the paper to address the minor typos mentioned in the review.",
"Comment32": "The RMSProp used in Section 4 in Mukkamala and Hein, ICML'17 is not the standard RMSProp but a modification in which the parameter used for computing the geometrical averages of the gradient entries squared changes with time. So there is no contradiction with this paper, that shows counterexamples for the standard algorithm in which that parameter is constant. ",
"Comment33": "Hello,    I tried implementing AMSGrad (here: https://colab.research.google.com/notebook#fileId=1xXFAuHM2Ae-OmF5M8Cn9ypGCa_HHBgfG) for the experiment on the stochastic optimization setting and obtain that x_t approaches -1 faster that on the paper but convergence seems less stable, so I was wondering about the specific values for other hyperparameters like the learning rate and epsilon which weren't mentioned, in my case I chose a learning of 1e-3 and an epsilon of 1e-8 which seems to be the standard value on most frameworks.",
"Comment34": "We thank you for your interest in our paper and for pointing out this missing detail. We use a decrease step size of alpha/sqrt(t) (as suggested by our theoretical analysis) for the stochastic optimization experiment. The use of decreasing step size leads to a more stable convergence to the optimal solution (especially in scenarios where the variance is reasonably high). We did not use epsilon in this particular experiment since the gradients are reasonably large (in other words, using a small epsilon like 1e-8 should produce more or less identical results). We will add these details in the next revision of our paper.",
"Abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
"Publication_Date": "2018/02/15",
"Readers": "everyone",
"URL": "https://openreview.net/pdf?id=ryQu7f-RZ"
},
{
"Title": "Synthetic And Natural Noise Both Break Neural Machine Translation",
"Keyword": "['NeuralMachineTranslation', 'Characters', 'Noise', 'AdversarialExamples', 'RobustTraining']",
"Rating1": 8,
"Review1": "This paper investigates the impact of noisy input on Machine Translation, and tests simple ways to make NMT models more robust.    Overall the paper is a clearly written, well described report of several experiments. It shows convincingly that standard NMT models completely break down on both natural *noise* and various types of input perturbations. It then tests how the addition of noise in the input helps robustify the charCNN model somewhat. The extent of the experiments is quite impressive: three different NMT models are tried, and one is used in extensive experiments with various noise combinations.    This study clearly addresses an important issue in NMT and will be of interest to many in the NLP community. The outcome is not entirely surprising (noise hurts and training and the right kind of noise helps) but the impact may be. I wonder if you could put this in the context of *training with input noise*, which has been studied in Neural Network for a while (at least since the 1990s). I.e. it could be that each type of noise has a different regularizing effect, and clarifying what these regularizers are may help understand the impact of the various types of noise. Also, the bit of analysis in Sections 6.1 and 7.1 is promising, if maybe not so conclusive yet.    A few constructive criticisms:    The way noise is included in training (sec. 6.2) could be clarified (unless I missed it) e.g. are you generating a fixed *noisy* training set and adding that to clean data? Or introducing noise *on-line* as part of the training? If fixed, what sizes were tried? More information on the experimental design would help.    Table 6 is highly suspect: Some numbers seem to have been copy-pasted in the wrong cells, eg. the *Rand* line for German, or the Swap/Mid/Rand lines for Czech. It's highly unlikely that training on noisy Swap data would yield a boost of +18 BLEU points on Czech -- or you have clearly found a magical way to improve performance.    Although the amount of experiment is already important, it may be interesting to check whether all se2seq models react similarly to training with noise: it could be that some architecture are easier/harder to robustify in this basic way.    [Response read -- thanks]  I agree with authors that this paper is suitable for ICLR, although it will clearly be of interest to ACL/MT-minded folks.",
"Comment11": "Thank you for the constructive feedback.   1. Noise setup: when training with noise, we replace the original training set with a new, noisy training set. The noisy training set has exactly the same number of sentences and words as the training set, but noise is introduced according to the description in Section 4. Therefore, we have one fixed noisy training set per each noise type. We’ll clarify the experimental design in the paper.     2. We had not thought to explore the relationship between the noise we are introducing as a corruption of the input and the training under noise paradigm you referenced. We might be mistaken, but normally, the corruption (e.g. Bishop 95) is in the form of small additive gaussian noise. It isn’t immediately clear to us whether discrete perturbation of the input like we have here is equivalent, but would love suggestions on analyses we might do to investigate this insight further.    3. Some cells in the mentioned rows in Table 6 were indeed copied from the French rows by error. We corrected the numbers and they are in line with the overall trends. Thank you for pointing this out.  The corrected Czech numbers are in the 20s and the best performing system is the Rand+Key+Real setting.    4. (To both reviewer 2 and 3) Regarding training other seq2seq models with noise: Our original intent was to test the robustness of pre-trained state-of-the-art models, but we also considered retraining them in this noisy paradigm. There are a number of design decisions that are involved here (e.g. should the BPE dictionary be built on the noisy texts and how should thresholds be varied?). That being said, we can investigate training using published parameter values, but worry these may be wholly inappropriate settings for the new noisy data.",
"Rating2": 7,
"Review2": "This paper investigates the impact of character-level noise on various flavours of neural machine translation. It tests 4 different NMT systems with varying degrees and types of character awareness, including a novel meanChar system that uses averaged unigram character embeddings as word representations on the source side. The authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables. They show that all NMT systems, whether BPE or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data. Unfortunately, they are not able to show any types of synthetic noise helping address natural noise. However, they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise.    This is a thorough exploration of a mostly under-studied problem. The paper is well-written and easy to follow. The authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty. The inclusion of so many character-based systems is very nice, but it is the inclusion of natural sources of noise that really makes the paper work. Their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others. In terms of negatives, it feels like this work is just starting to scratch the surface of noise in NMT. The proposed meanChar architecture doesn’t look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isn’t extremely satisfying. Furthermore, the use of these replacement tables means that even when the noise is natural, it’s still kind of artificial. Finally, this paper doesn’t seem to be a perfect fit for ICLR, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *ACL conference.    Regarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial. First of all, errors learned from the noisy data sources are constrained to exist within a word. This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words. Second, if I understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data. This seems worse than estimating the frequency of the error and applying them stochastically (or trying to learn when an error is likely to occur). I feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.    Also, it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data (Table 6). I realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner. On a related note, the line in the abstract stating that “... a character convolutional neural network  is able to simultaneously learn representations robust to multiple kinds of noise” implies that the other (non-charCNN) architectures could not learn these representations, when in reality, they simply weren’t given the chance.    Section 7.2 on the richness of natural noise is extremely interesting, but maybe less so to an ICLR audience. From my perspective, it would be interesting to see that section expanded, or used as the basis for future work on improve architectures or training strategies.    I have only one small, specific suggestion: at the end of Section 3, consider deleting the last paragraph break, so there is one paragraph for each system (charCNN currently has two paragraphs).    [edited for typos]",
"Comment21": "Thank you for the useful feedback.  We agree that noisy input in neural machine translation is an under-studied problem.     Responses to specific comments:  1. We agree that our work only starts to scratch the surface of noise in NMT and believe there’s much more to be done in this area. We do believe that it’s important to initiate a discussion of this issue in the ICLR community, for several reasons: (a) we study word and character representations for NMT, which is in line with the ICLR representation learning theme; (b) ICLR audience is very interested in neural machine translation and seminal work on NMT has been published in ICLR (e.g., Bahdanau et al.’s 2015 paper on attention in NMT); (c) ICLR audience is very interested in noise and adversarial examples, as evidenced by the plethora of recent papers on the topic. As reviewer 1 says, even though there are no fancy new methods in the paper, we believe that this kind of research belongs in ICLR.    2. We agree that meanChar may not be the ideal architecture for capturing noise, but it’s a simple, structure-invariant representation that works reasonably well. We have tried several other architectures, including a self-attention mechanism, but haven’t been able to improve beyond it. We welcome more suggestions and can include those negative results in new drafts of the paper.    3. Training with noise has its limitations, but it’s an effective method that can be employed by NMT providers and researchers easily and impactfully, as pointed out by reviewer 1.     4. In this work, we focus on word-level noise. Certainly, sentence-level noise is also important to learn, and we’d like to see more work on this. We’ll add this as another direction for future work. Note that while charCNN may have some advantage in dealing with word-level noise, it too suffers from increasing amounts of noise, similar to the other models we studied.    5. Applying noise stochastically based on frequency in available corpora is an interesting suggestion, that can be done for the natural noise, but not so clear how to apply for synthetic noise. We did experiment with increasing amounts of noise (Figure 1), but we agree there’s more to be done. We’ll add this as another future work.      6. (To both reviewer 2 and 3) Regarding training other seq2seq models with noise: Our original intent was to test the robustness of pre-trained state-of-the-art models, but we also considered retraining them in this noisy paradigm. There are a number of design decisions that are involved here (e.g. should the BPE dictionary be built on the noisy texts and how should thresholds be varied?). That being said, we can investigate training using published parameter values, but worry these may be wholly inappropriate settings for the new noisy data.    7. We’ll modify the abstract to not give the wrong impression regarding what other architectures can learn.     8. We included section 7.2 to demonstrate why synthetic noise is not very helpful in dealing with natural noise, as well as to motivate the development of better architectures.     9. We’ll correct the other small issues pointed to. ",
"Comment22": "Thanks for your thoughtful response to my review.",
"Rating3": 7,
"Review3": "This paper empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural. The results are not surprising:    * NMT is terrible with noise.    * But it improves on each noise type when it is trained on that noise type.    What I like about this paper is that:    1) The experiments are very carefully designed and thorough.    2) This problem might actually matter. Out of curiosity, I ran the example (Table 4) through Google Translate, and the result was gibberish. But as the paper shows, it’s easy to make NMT robust to this kind of noise, and Google (and other NMT providers) could do this tomorrow. So this paper could have real-world impact.    3) Most importantly, it shows that NMT’s handling of natural noise does *not* improve when trained with synthetic noise; that is, the character of natural noise is very different. So solving the problem of natural noise is not so simple… it’s a *real* problem. Speculating, again: commercial MT providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper, but at much larger scale. So these methods could be applied in the real world. (It would be excellent if an outcome of this paper was that commercial MT providers answered it’s call to provide more realistic noise by actually providing examples.)    There are no fancy new methods or state-of-the-art numbers in this paper. But it’s careful, curiosity-driven empirical research of the type that matters, and it should be in ICLR.",
"Comment31": "Thank you for the useful feedback.     1. We agree that the topic has real-world impact for MT providers and will emphasize this in the conclusions.     2. We would love to see MT providers use noisy data and we agree that the community would benefit from access to more noisy examples. ",
"Abstract": "Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise. ",
"Publication_Date": "2018/02/15",
"Readers": "everyone",
"URL": "https://openreview.net/pdf?id=BJ8vJebC-"
},
{
"Title": "Multi-scale Dense Networks For Resource Efficient Image Classification",
"Keyword": "['EfficientLearning', 'BudgetedLearning', 'DeepLearning', 'ImageClassification', 'ConvolutionalNetworks']",
"Rating1": 10,
"Review1": "This paper introduces a new model to perform image classification with limited computational resources at test time. The model is based on a multi-scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016), but with dense connections (Huang et al., 2017) and with a classifier at each layer. The multiple classifiers allow for a finer selection of the amount of computation needed for a given input image. The multi-scale representation allows for better performance at early stages of the network. Finally the dense connectivity allows to reduce the negative effect that early classifiers have on the feature representation for the following layers.  A thorough evaluation on ImageNet and Cifar100 shows that the network can perform better than previous models and ensembles of previous models with a reduced amount of computation.    Pros:  - The presentation is clear and easy to follow.  - The structure of the network is clearly justified in section 4.  - The use of dense connectivity to avoid the loss of performance of using early-exit classifier is very interesting.  - The evaluation in terms of anytime prediction and budgeted batch classification can represent real case scenarios.  - Results are very promising, with 5x speed-ups and same or better accuracy that previous models.  - The extensive experimentation shows that the proposed network is better than previous approaches under different regimes.    Cons:  - Results about the more efficient densenet* could be shown in the main paper    Additional Comments:  - Why in training you used logistic loss instead of the more common cross-entropy loss? Has this any connection with the final performance of the network?  - In fig. 5 left for completeness I would like to see also results for DenseNet^MT and ResNet^MT  - In fig. 5 left I cannot find the 4% and 8% higher accuracy with 0.5x10^10 to 1.0x10^10 FLOPs, as mentioned in section 5.1 anytime prediction results  - How the budget in terms of Mul-Adds is actually estimated?    I think that this paper present a very powerful approach to speed-up the computational cost of a CNN at test time and clearly explains some of the common trade-offs between speed and accuracy and how to improve them. The experimental evaluation is complete and accurate. ",
"Comment11": "Thank you for the encouraging comments!     # DenseNet*  We have included the DenseNet* results in the main paper as suggested. We placed this network originally in the appendix to keep the focus of the main manuscript on the MSDNet architecture, and it was introduced for the first time in this paper (although as a competitive baseline).    # logistic loss  We actually used the cross entropy loss in our experiments. We have fixed this sentence. Thanks for pointing out.    # DenseNet^MC and ResNet^MC on ImageNet (left panel of Fig.5)  We observed that DenseNet^MC and ResNet^MC are two of the weakest baselines on both CIFAR-10 and CIFAR-100 datasets. Therefore, we thought their results on ImageNet probably won’t add much to the paper. We can add these results in a later version.    # improvements in the anytime setting  It should be 4% and 8% higher accuracy when the budget ranges from 0.1x10^10* to 0.3x10^10* FLOPs. We have corrected it in the updated version.    # actually budget  For many devices, e.g., ARM processor, the actual inference time is basically a linear function of the number of Mul-Add operations. Thus in practice, given a specific device, we can estimate the budget in terms of Mul-Add according to the real time budget.",
"Rating2": 7,
"Review2": "This paper presents a method for image classification given test-time computational budgeting constraints.  Two problems are considered:  *any-time* classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images.  A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features.  In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage.  Evaluations are performed on ImageNet and CIFAR-100.    I would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well?  Also on p.6 I'm not entirely clear on how the *network reduction* is performed --- it looks like finer scales are progressively dropped in successive blocks, but I don't think they exactly correspond to those that would be needed to evaluate the full model (this is *lazy evaluation*).  A picture would help here, showing where the depth-layers are divided between blocks.    I was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result:  It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation.  So this isn't really a hard constraint (just an expected result for batches that are large and varied enough).  This is fine, but could perhaps be pointed out if that is indeed the case.    Overall, this seems like a natural and effective approach, and achieves good results.",
"Comment21": "Thanks for the positive comments.    # MC baselines on ImageNet  We exclude these results in our current version as we observed that they are far from competitive on both CIFAR-10 and CIFAR-100. We are testing the MC baselines on ImageNet, and will include it in a later version, but won’t expect them to be strong baselines.    # network reduction  The ‘network reduction’ is a design choice to reduce redundancy in the network, while ‘lazy evaluation’ is a strategy to avoid redundant computations. We have added a figure (Figure 9) in the appendix to illustrate the reduced network as suggested.     # batched budgeted evaluation  Thanks for pointing out. We have emphasize that the notion of budget in this context is a “soft constraint” given a large batch of testing samples.",
"Rating3": 8,
"Review3": "This work proposes a variation of the DenseNet architecture that can cope with computational resource limits at test time. The paper is very well written, experiments are clearly presented and convincing and, most importantly, the research question is exciting (and often overlooked).     My only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al. (2017). The authors add a hierarchical, multi-scale structure and show that DenseNet can better cope with it than ResNet (e.g., Fig. 3). They investigate pros and cons in detail adding more valuable analysis in the appendix. However, this work is basically an extension of the DenseNet approach with a new problem statement and additional, in-depth analysis.       Some more minor comments:     - Please enlarge Fig. 4.   - I did not fully grasp the details in the first *Solution* paragraph on P5. Please extend and describe in more detail.     In conclusion, this is a very well written paper that designs the network architecture (of DenseNet) such that it is optimized to include CPU budgets at test time. I recommend acceptance to ICLR18.      ",
"Comment31": "Thanks for positive comments.     # difference to DenseNet  Although dense connectivity is one of the two key components in our MSDNet, this paper is quite different from the original DenseNet paper: (1) in this paper we tackle a very different problem, the inference of deep models with computational resource limits at test time; (2) we show the multi-scale features are crucial for learning accurate early classifiers. Finally, MSDNet yields 2x to 5x faster inference speed than DenseNet under the batch budgeted setting.    # minors  Thanks for these suggestions. We have incorporated them in the updated version.",
"Abstract": "In this paper we investigate image classification with computational resource limits at test time. Two such settings are: 1. anytime classification, where the network’s prediction for a test example is progressively updated, facilitating the output of a prediction at any time; and 2. budgeted batch classification, where a fixed amount of computation is available to classify a set of examples that can be spent unevenly across “easier” and “harder” inputs. In contrast to most prior work, such as the popular Viola and Jones algorithm, our approach is based on convolutional neural networks. We train multiple classifiers with varying resource demands, which we adaptively apply during test time. To maximally re-use computation between the classifiers, we incorporate them as early-exits into a single deep convolutional neural network and inter-connect them with dense connectivity. To facilitate high quality classification early on, we use a two-dimensional multi-scale network architecture that maintains coarse and fine level features all-throughout the network. Experiments on three image-classification tasks demonstrate that our framework substantially improves the existing state-of-the-art in both settings.",
"Publication_Date": "2018/02/15",
"Readers": "everyone",
"URL": "https://openreview.net/pdf?id=Hk2aImxAb"
}
]