[
{
"Title": "Unsupervised Video-to-video Translation",
"Keyword": "['GenerativeAdversarialNetworks', 'ComputerVision', 'DeepLearning']",
"Rating1": 3,
"Review1": "This paper present a spatio-temporal (i.e., 3D version) of Cycle-Consistent Adversarial Networks (CycleGAN) for unsupervised video-to-video translation. The evaluations on multiple datasets show the proposed model is better able to work for video translation in terms of image continuity and frame-wise translation quality.     The major contribution of this paper is extending the existing CycleGAN model from image-to-image translation and video-to-video translation using 3D convolutional networks, while it additionally proposes a total penalty term to the loss function. So I mainly concern that such contribution might be not enough for the ICLR quality. ",
"Rating2": 4,
"Review2": "1) Summary  This paper proposes a 3D convolutional neural network based architecture for video-to-video translation. The method mitigates the inconsistency problem present when image-to-image translation methods are used in the video domain. Additionally, they present a study of ways to better setting up batched for the learning steps during networks optimization for videos, and also, they propose a new MRI-to-CT dataset for medical volumetric image translation. The proposed method outperforms the image-to-image translation methods in most measures.        2) Pros:  + Proposed network architecture mitigates the pixel color discontinuity issues present in image-to-image translation methods.  + Proposed a new MRI-to-CT dataset that could be useful for the community to have a benchmark on medical related research papers.    3) Cons:  Limited network architecture:  - The proposed neural network architecture is limited to only generate the number of frames it was trained to generate. Usually, in video generation / translation / prediction we want to be able to produce any length of video. I acknowledge that the network can be re-used to continue generating number of frames that are multiples of what the network was trained to generate, but the authors have not shown this in the provided videos. I would be good if they can provide evidence that this can be done with the proposed network.    Short videos:  - Another limitation that is related to the previously mentioned issue is that the videos are short, which in video-to-video translation, it should not be difficult to generate longer videos. It is hard to conclude that the proposed method will work for large videos from the provided evidence.    Lack of baselines:  - A paper from NVIDIA Research on video-to-video synthesis [1] (including the code)  came out about a month before the ICLR deadline. It would be good if the authors can include comparison with this method in the paper revision. Other papers such as [2, 3] on image-to-image translation are available for comparison. The authors simply say such methods do not work, but show no evidence in the experimental section. I peeked at some of the results in the papers corresponding websites, and the videos look consistent through time. Can the authors comment on this if I am missing something?      Additional comments:  The authors mention in the conclusion that this paper proposes “a new computer vision task or video-to-video translation, as well as, datasets, metrics and multiple baselines”. I am not sure that video-to-video translation is new, as it has been done by the papers I mention above. Maybe I am misunderstanding the statement? If so, please clarify. Additionally, I am not sure how the metrics are new. Human evaluation has been done before, the video colorization evaluation may be somewhat new, but I do not think it will generalize to tasks other than colorization. Again, If I am misunderstanding the statement, please let me know in the rebuttal.        4) Conclusion  The problem tackled is a difficult one, but other papers that are not included in experiments have been tested on this task. The proposed dataset can be of great value to the community, and is a clearly important piece of this paper. I am willing to change the current score if they authors are able to address the issues mentioned above.      References:  [1] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. *Video-to-Video Synthesis*. In NIPS, 2018.  [2] Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz. Multimodal Unsupervised Image-to-Image Translation. In ECCV, 2018.  [3] Ming-Yu Liu, Thomas Breuel, Jan Kautz. Unsupervised Image-to-Image Translation Networks. In NIPS, 2017.",
"Rating3": 4,
"Review3": "This paper proposes a spatio-temporal 3D translator for the unsupervised image-to-image translation task and a new CT-to-MRI volumetric images translation dataset for evaluation. Results on different datasets show the proposed 3D translator model outperforms per-frame translation model.    Pros:  * The proposed 3D translator can utilize the spatio-temporal information to keep the translation results consistent across time. Both color and shape information are preserved well.   * Extensive evaluation are done on different datasets and the evaluation protocols are designed well. The paper is easy to follow.    Cons:  * The unsupervised video-to-video translation task has been tested by previous per-frame translation model, e.g. CycleGAN and UNIT. Results can be found on their Github project page. Therefore, unsupervised video-to-video translation is not a new task as clarified in the paper, although this paper is one of the pioneers in this task.   * The proposed 3D translator extend the CycleGAN framework to video-to-video translation task with 3D convolution in a straightforward way. The technical novelty of the paper is limited for ICLR.  I think the authors are working on the right direction, but lots of improvement should be done.  * As to Table 4, I am confused about the the per-frame pixel accuracy results. Does the 3D method get lower accuracy than 2D method?  * As to the GTA segmentation->video experiments, the 3D translator seems cause more artifacts than the 2D method (page 11,12). Also, the title of the figure on page 11 should both be “GTA segmentation->video”    Overall, the technical innovation of this paper is limited and the results are not good enough. I vote for rejection.",
"Abstract": "Unsupervised image-to-image translation is a recently proposed task of translating an image to a different style or domain given only unpaired image examples at training time. In this paper, we formulate a new task of unsupervised video-to-video translation, which poses its own unique challenges. Translating video implies learning not only the appearance of objects and scenes but also realistic motion and transitions between consecutive frames. We investigate the performance of per-frame video-to-video translation using existing image-to-image translation networks, and propose a spatio-temporal 3D translator as an alternative solution to this problem. We evaluate our 3D method on multiple synthetic datasets, such as moving colorized digits, as well as the realistic segmentation-to-video GTA dataset and a new CT-to-MRI volumetric images translation dataset. Our results show that frame-wise translation produces realistic results on a single frame level but underperforms significantly on the scale of the whole video compared to our three-dimensional translation approach, which is better able to learn the complex structure of video and motion and continuity of object appearance. ",
"Publication_Date": "2018/09/27",
"Readers": "everyone",
"URL": "https://openreview.net/pdf?id=SkgKzh0cY7"
},
{
"Title": "Favae: Sequence Disentanglement Using In- Formation Bottleneck Principle",
"Keyword": "['DisentangledRepresentationLearning']",
"Rating1": 5,
"Review1": "This paper presents a new approach to learning disentangled representations of sequential data. The model, FAVAE, is based on the information bottleneck framework (Alemi et al, 2016; Achille et al, 2016) and extends the recent beta-VAE (Higgins et al, 2017) and CCI-VAE (Burgess et al, 2017) work by changing the encoder/decoder to a Quasi-Recurrent Neural Network (QRNN) and adding multiple latents through a ladder VAE approach (Zhao et al, 2017). The authors demonstrate that their approach is able to learn a more disentangled representation than the limited set of baselines on three toy datasets.    The authors address a very important problem, since the ability to learn disentangled representations of sequential data can have far reaching consequences, as the authors rightfully point out in their introduction. I also like the approach that the authors are taking, which appears to be very general and does not seem to involve the need to have any domain knowledge. However, the paper could be improved by clarifying certain key parts and extending the experimental section, which is currently not quite as convincing as I would have hoped.    The summary of my concerns is the following:     I would like to see more baseline comparisons 1) to an FAVAE with a different recurrent encoder/decoder; 2) to at least one other approach to disentangled representation learning on sequences; 3) an FAVAE without the capacity increase. I would also like to see all the baselines run on a non toy dataset of video data. Finally, I would like to see an expanded discussion of what the different latent variables at the different levels of the ladder architecture are learning. I recommend that the authors remove the MIE metric and shorten Section 3 to make space for the expanded experiments section.    I do hope that the authors are able to address my concerns, because their method has a lot of potential and I am excited to see where they take it during the rebuttal period. Please see the more detailed comments below:    1) Section 4.2 should be expanded to include a more detailed description of QRNN. This is one of the key modifications of FAVAE compared to CCI-VAE or beta-VAE, and it is currently not clear how QRNN works unless one reads the original paper referenced. The current paper needs to be self contained. It would also be nice to get a better understanding why QRNN was chosen over an LSTM or a GRU. It would be useful to see the results of the baseline experiments with an LSTM compared to the equivalent QRNN-based version of FAVAE.    2) Why do the authors introduce the new MIE metric? The reported results do not show a significant problem with the MIG metric, and the need for the new MIE metric is not well motivated. If the authors insist on introducing this new metric, it is important to demonstrate cases where MIG fails and MIE performs better. Otherwise I would advise removing the new metric and using the space to expand section 4.2 instead.     Another point on the metric, Eq. 16 seems to be missing a term that goes over latents z_j. I assume there should be either a max_j or a mean_j in front of the I(z_j; v_k) term in the first part of Eq. 16?    3) The related work section should be re-worded. Currently it reads as if the other approaches do not optimise a loss function at all. It would also be good to include one of the mentioned models as a baseline, and to run both FAVAE and one of the previously introduced models for disentangled sequential representation learning on a more complex dataset of video data.    4) In section 7.1, it would be good to expand the discussion of how latent traversal plots are obtained. In particular, I do not understand how the different latent variables in the ladder architecture of FAVAE are traversed. In general, it would also be nice to expand the discussion of what the different latents at the different ladder stages learn, and how the number of ladder stages affects the nature of learnt representations.    5) In terms of the baselines, it would be good to see the full ablation study. The way I see it, FAVAE has three modifications on the standard variational approaches: 1) the use of a recurrent encoder/decoder (R); 2) the use of the ladder architecture (L); and 3) the use of the capacity constraint objective (C). Currently the authors show the results of the R--, R-C, and RLC conditions. I would also like to see the results of the RL- condition (where beta=1 and C=0).    6) In terms of the results presented in Tbl. 1, it would be nice to include the hyperparameters that the authors have swept over in the appendix, as well as a table of the architecture parameters and the best hyperparameters found for the models presented in the Experiments section. In Tbl.1 it is currently unclear how many seeds the authors used to calculate the standard deviations. The units of the standard deviations presented are also not clear. Finally, it is not clear whether the differences presented in the table are significant.    7) It would be useful to include some details of the 2D Wavy Reaching dataset in the main text, even if it is just listing the nature of the 5 factors.    8) It would be useful to expand the section that talks about the different settings of C explored (page 7, paragraph 2). Currently the point that the authors are trying to make in that paragraph and Fig. 4 is not clear. I would also recommend having beta in Fig. 4 on linear rather than log scale, as the log scale seems to be somewhat confusing.          Minor points:    -- Page 2/paragraph 2: used disentangle -> used to disentangle  -- P4/p5: FAVAE is disentangled representation -> is for(?) disentangled representation  -- P6/p1: the priors time dependency -> the priors' time dependency  -- P7/p1: scores for2D -> scores for 2D  -- P7/p4: MIE score (gray curve) -> MIE score (green(?) curve)",
"Comment11": "1) We would like to compare with LSTM and GRU case, but we failed to reconsturct timeseries data with recurrent neural network, so we could not make meaningful comparison.  The reason for using time convolution is to combine several models with ladder + time conversion + CCI-VAE or beta-VAE, so we want to simplify time series processing.    2) We reply common to all reviewers: comment 4.  3) We reply common to all reviewers: comment 2.  4) We reply common to all reviewers: comment 3.  5) We agree with the opinion that it is important to estimate the effects of beta, c and ladder individually. We add beta=1, C=0 case in Table 1.  6) We add information of standard error in Table1 (the number of seed is 10). Unfortunately, we could not discuss statistically significant differences here.  7) Added explanation of factor of 2D wavy Reaching dataset in Fig. 5.  8) Does it mean that it is easier to understand by plotting in detail about β = 100 in Fig. 4B?  We used logarithmic scales because we wanted to claim the result that reconstruction worsens as β is increased and MIG worsens as a result, unless c is adjusted to the optimum value. If the figure on the linear scale near β = 100 is better, update Fig. 4B (Should we also update fig 4A?).",
"Rating2": 6,
"Review2": "This paper proposes an extension to VAE to model sequential datasets and extract disentangled representations of their evolution.  It consists of a straight extension of CCI-VAE (Burgess et al 2018) to accept sequential data, combining it with a Ladder VAE architecture (VLAE, Zhao et al 2017).  They show early results on fitting toy domains involving 2D points moving in space (reaching, reaching in sub-sequences with complex dynamics, gripper domain).    Overall, I think this is an interesting piece of work, which proposes a good model extension and assessment of its characteristics. The model is well presented, the different components are sufficiently motivated and they perform just enough experiments to showcase the effectiveness of their method, although with some reservations.    Critics:  1. The comparison to Beta-VAE is a straw man, and I’m not sure it’s a valid way to introduce your model. You are basically saying that treating sequential data as if it was non-sequential is bad, which is clearly not surprising? Hence any comparison with Beta-VAE that you show, Figure 1 and Figure 3, are not appropriate (the caption of Fig 1 is particularly bad in that aspect). A more correct comparison would be to directly feed x_1:T to a Beta-VAE and see what happens, maybe with a causal time-convolution as well if you want to avoid 3D filters.  2. You are also not comparing to the FHVAE model you present in your Introduction, which would have been nice to see, given that your model is simpler and requires less supervision. Does FAVAE perform better than these?  3. Section 3 could use a citation to Esmaeili et al 2018, which breaks out the ELBO even further and compares multiple models in a really nice way (e.g. Table A.2). Overall Section 2 and 3 feel a bit long and pedantic, you could just point people to the original papers and move some of the justification to Appendix (e.g. the IB arguments are not that required for your model. ).  The main point you want to put across is that you want to have your “z” compress a full trajectory x_1:T, under a single KL pressure (i.e. last sentence of Section 4).  4. Figure 3 was hard to interpret at first, specifically for panels b and c. Maybe if you showed the “sampled trajectory” only once in another plot it would make it clearer.  5. Time-convolution seems to wrongly be using the opposite indexing? With z_tk = \\sum_{p=0}^{H} x_{t+p}, you have an anti-causal filter which looks at the future x_t’ for a z_t? That does not sound right? Also, you should call these “causal convolutions”, which is the more standard term.  6. The exact format of the observations was never clearly explained. From 7.1 I understood that you input 2D positions into the models, but what about the Gripper?  As you’re aware, Beta-VAE and others usually get RGB images as inputs, hence you should make that difference rather clear. This is a much simpler setting you’re working in.  7. Did you anneal the C as was originally proposed in Burgess et al 2018? With which schedule? This was not clear to me. The exact choices of C for the different Ladder levels lacked support as well. An overall section in the Appendix about the parameter ranges you tried, the architectures, the observation specifications, the optimisation schedule etc etc would be useful.  8. I appreciate the introduction of the MIE metric, which seems to slightly improve over MIG in a meaningful way. However, it would be good to show situations where the two metrics disagree and why MIE is better, because in the current experiments this is unclear.  9. Overall the Gripper experiments seem to merit a more complete assessment. Figure 7 was hard to understand, and I am not sure it shows clearly any disentangled factors. Its caption was strange too (what are the “(1, 8)”, “(2, 1)” things referring to?).   10. I would have liked more interpretation and comments on why the Ladder is needed, and why FAVAE (without Ladder and C) does so badly in Table 1.  11. It would be good to know if you really find that the different levels of the Ladder end up extracting different time scales, as you originally claim it can. There are no results supporting this assumption in the current version.  12. Figure 4B uses a bad scale, which makes it hard to assess what happens between the two C conditions for Beta \\approx 100, where they seem to differ the most in Fig 4A.  13. Figure 5 could use titles/labels indicating which “generative factors” you think are being represented. Just compare them to your Figure 8 in Appendix.  14. Figure 6 MIE scores look all within noise between models considered. How sensitive is the metric to actual differences in the disentanglement?    Overall, I think this is an interesting improvement to disentangled representations learning, but suffers a bit from early experimental results. I would still like it to be shown at ICLR though as it really fits the venue.  I'm happy to improve my score given some improvements on the points mentioned above.    References:  - Burgess et al., 2018: https://arxiv.org/abs/1804.03599  - Zhao et al., 2017: https://arxiv.org/abs/1702.08396   - Esmaeili et al., 2018: https://arxiv.org/abs/1804.02086 ",
"Comment21": "1) We reply common to all reviewers: comment 1. We show Fig. 1 and 3 to clarify that the FAVAE extract disentangled representations and beta-VAE extract disentangled representations are different. The baseline model in quantitative evaluation experiment is compared with sequential model(Time convolution AE).  2) reply common to all reviewers:1.   3) > The main point you want to put across is that you want to have your “z” compress a full trajectory x_1:T, under a single KL pressure (i.e. last sentence of Section 4).    We agree with the advice. We will modify for space.    4) Since sample trajectory is confusing, we will delete it in Fig. 3.  5) Thank you for pointing out the mistake. We modified eq. 15. Our model does not use causal convolution, 1D convolution is used. The reason why Causal convolution is not used is because 1D conv is reasonable as it can use the information of the previous and subsequent time steps. Also we do not need to use x_t for recurrent at generation, so we can use without causal convolution.    6) We use the value such as position in Gripper dataset, not the image. We added information on Gripper input x to Appendix B.2. We should clarify that there is a difference between the image and position input, so we added to Sec. 7.3 that we don't use images as input.  7) Yes, The scheduling c is originally proposed in Burgess et al 2018. We linear scheduling from 0 to target c as [20,1,5] in 10000 step (all experiments same). We used the same C in the same ladder(e.g. 1st ladder has 8 z dimension, it's c=20. 2nd ladder has 4 z dimension, it's c=1, 3rd ladder has 2 z dimension, it's c=5)   8) We reply common to all reviewers: comment 4.  9) We improve the cation in Fig. 7.  10) C is an indicator of how much information is left when compressing data. Since 2D Reaching was small in dataset information, the optimum value of C was almost 0. So for simple data FAVAE will be not bad results with C = 0.  11) We reply common to all reviewers: comment 3.  12) Does it mean that it is easier to understand by plotting in detail about β = 100 in Fig. 4B?  13) Added explanation of factor of 2D wavy Reaching dataset in Fig. 5.  14) > Figure 6 MIE scores look all within noise between models considered. How sensitive is the metric to actual differences in the disentanglement?  In Fig. 6, only at higher ladder 1 have large error case (there was a case like loss> 57 although loss = 0.8 in general). We show the average of loss and MIG in each case (7 out of 10 succeeded).     ||loss|MIG|  |:--:|:--:|:--:|  |success(7)|0.85|0.17|  |fail(3)|57.3|0.071239|",
"Comment22": "Thanks to the authors for their responses and hard work in improving their results.    1) The new versions of sections 2, 3 and 4 are much improved, great work!  2) I still think Figure 1 is too much space for not a very important point to make, no-one should expect Beta-VAE in its standard form to extract temporal information. It is less problematic now however.  3) Figure 5 is now clearer, thanks. Figure 4 is nicer too, but I’m not extremely sure what point you’re trying to put across with it. Does it inform your choice of beta? It is not clear from reading the text.  4) I think your results on the Sprites dataset are really promising, and actually fit the paper much better than your previous Gripper experiment. They do seem early though, so I’d recommend continuing on them, and try to give a better overview of the effect of the latents, and of which levels of “semantic factors” are captured by the different ladder levels. I think with some work and clean up they could make really strong points.  5) Related to that, I think it is still unclear in the current version exactly what is the effect/responsibilities of each ladder layer. Fig 7 goes in the good direction, but I was confused by Table 2 in Appendix B, it does not really tell a simple story of “highest/slowest ladder controls the longer-term/constant factors, vs lowest ladder controls the details of the trajectory”. At least the text does not currently express that simply.    Overall, I think this is a nice contribution, but it would still use some more work to get it to an appropriate level for publication.",
"Comment23": "> 3) Figure 5 is now clearer, thanks. Figure 4 is nicer too, but I’m not extremely sure what point you’re trying to put across with it. Does it inform your choice of beta? It is not clear from reading the text.    In the 3rd paragraph in Sec 7.2, we state that it is better to use C because when increasing Beta without introducing C, reconstruction loss gets bigger.    > 4) I think your results on the Sprites dataset are really promising, and actually fit the paper much better than your previous Gripper experiment. They do seem early though, so I’d recommend continuing on them, and try to give a better overview of the effect of the latents, and of which levels of “semantic factors” are captured by the different ladder levels. I think with some work and clean up they could make really strong points.    We added the explanation how the semantic factors were extracted into certain ladders in the Sprite dataset experiments. However, as far as we tried in our experiments, we couldn't see the tendency that certain factor concentrates in certain ladder. We think that this is because it is not clear that Sprite dataset is composed of multiple abstract levels of dynamic factors.      > 5) Related to that, I think it is still unclear in the current version exactly what is the effect/responsibilities of each ladder layer. Fig 7 goes in the good direction, but I was confused by Table 2 in Appendix B, it does not really tell a simple story of “highest/slowest ladder controls the longer-term/constant factors, vs lowest ladder controls the details of the trajectory”. At least the text does not currently express that simply.     We modified the texts in Appendix B. Here we state that the long time dependency is expressed in the 3rd ladder which passes time convolution the most, and short time dependency is expressed in the 1st ladder. In 2D Wavy Reaching dataset, there is distinct difference between factors of long and short time dependency. The goal of the trajectory is the factor which affect the entire trajectory, and other factors affect half length of the trajectory (Fig. 9). In our experiment the goal of the trajectory which affect the entire trajectory tended to be expressed in the 3rd ladder.",
"Rating3": 4,
"Review3": "The paper proposes the factorized action variational autoencoder (FAVAE) as a new model for learning disentangled representations in high-dimensional sequences, such as videos. In contrast to earlier work that encouraged disentanglement through a carefully-designed dynamical prior, the authors propose a different encoder-decoder architecture combined with a modified loss function (Beta-VAE with a “shift C scheme”). As a result, the authors claim that their approach leads to useful disentangled representation learning in toy video data, and in data taken from a robotic task.    The paper appears to combine multiple ideas, which are not cleanly studied in isolation from each other. Several claims may be a bit oversold, such as potential applications for stock price data. But more importantly, the reasons why I don’t recommend accepting the paper are the following ones:    Lack of clarity:  I found that the paper lacks clarity in its presentation. Equation 7 presents a model that seems to have only a latent variable z without time dependence, but how can dynamic and static factors be separately controlled? I don't see this question addressed in the experiments. Also, what is the significance of the model architecture (ladder network) as compared to the modified loss function?  Furthermore, Fig. 7 is hard to read.    Lack of Experiments:  The currently presented experiments are all on rather simple data. I recommend extending the experiments to the Sprites data set, used in (Li and Mandt 2018), or to speech data. Also, the paper lacks comparisons to the recently proposed disentangled representation learning models (FHVAE and Disentangled Sequential Autoencoder).    While it is apparent that the model achieved some clustering, it is unclear to me if the final goal of separately controlling for static and dynamic aspects was really reached.",
"Comment31": "1. > Equation 7 presents a model that seems to have only a latent variable z without time dependence, but how can dynamic and static factors be separately controlled?    Since x_1: T is encoded in z, compress both static factors and dynamic factors to z. Total correlation of z decreases according to 2nd term of eq.5. That is, z is pressured to become independent. If the dynamic factor and the static factor are independent, it is possible to separate the static factor and the dynamic factor. Since FAVAE is automatically separated by pressure, separation cannot be controlled, but there is a merit that there is no need to give label information like FHVAE.    2. We reply common to all reviewers: comment 1 and 2.",
"Abstract": "A state-of-the-art generative model, a ”factorized action variational autoencoder (FAVAE),” is presented for learning disentangled and interpretable representations from sequential data via the information bottleneck without supervision. The purpose of disentangled representation learning is to obtain interpretable and transferable representations from data. We focused on the disentangled representation of sequential data because there is a wide range of potential applications if disentanglement representation is extended to sequential data such as video, speech, and stock price data. Sequential data is characterized by dynamic factors and static factors: dynamic factors are time-dependent, and static factors are independent of time. Previous works succeed in disentangling static factors and dynamic factors by explicitly modeling the priors of latent variables to distinguish between static and dynamic factors. However, this model can not disentangle representations between dynamic factors, such as disentangling ”picking” and ”throwing” in robotic tasks. In this paper, we propose new model that can disentangle multiple dynamic factors. Since our method does not require modeling priors, it is capable of disentangling ”between” dynamic factors. In experiments, we show that FAVAE can extract the disentangled dynamic factors.",
"Publication_Date": "2018/09/27",
"Readers": "everyone",
"URL": "https://openreview.net/pdf?id=Hygm8jC9FQ"
},
{
"Title": "Beyond Winning And Losing: Modeling Human Motivations And Behaviors With Vector-valued Inverse Reinforcement Learning",
"Keyword": "[]",
"Rating1": 5,
"Review1": "This paper studies inverse reinforcement learning with a vector-valued setting. A key motivation of the paper, as suggested by its title, is to incorporate and analyze the complex human motivations.    The proposed setting seems new to me, although vectored-valued rewards and Pareto optimality have been studied in the context of RL. The biggest issue of this paper, in my opinion, is it doesn't properly support its claim that it improves the understanding of the agents' motivations and the reward functions. Details comments / questions are listed below.    - Pareto dominance is a rather weak relation. When the number of criteria increases, it is less likely one alternative dominates another. In this case, the binary comparisons defined in Sec. 2.1 becomes less discriminative. Is this a problem to the proposed method?    - Pareto dominance and vector-valued rewards have been studied in preference-based reinforcement learning, such as Fürnkranz et al. 2012 @ MLJ and Cheng et al. 2011 @ ECML.     - Please fix the citation style in the paper and use \\citep and \\citet properly.     - The empirical study in this paper doesn't properly support the authors' claim. (1) It's questionable to assume the actions of a player in an online game are optimal or even rational. (2) The results presented in Figure 2 is hard to read and the differences look minor. (3) Maybe I miss it, but has Table 2 been referenced and explained in the paper?",
"Comment11": "Thank you for the review! The most important clarification that we would like to make, is that *Pareto dominance is a rather weak relation* makes the model rather strong. That is because the dominance relation is the assumption of the IRL models, and weak assumptions are desired. We believe that justifies our motivation of studying the Pareto dominance in the IRL regime.    On the empirical study, we agree that presenting the algorithm on only the real-world environments may depend on the rational assumption. In fact, we are aware of that the diversity in action originates from both the diversity of the agents' objective and their optimality (or even rationality). There is not too much one can resolve in the real-world dataset, but we can test the algorithm on the well-known RL environment and show the performance. We are also working on improving the writing quality and thanks for pointing those out.",
"Rating2": 4,
"Review2": "This paper presents NMBM, a general inverse reinforcement learning (IRL) model that considers multifaceted human motivations. The authors have motivated and proposed the algorithm (Section 2 and 3), and demonstrated some experiment results based on a real-world dataset (WoWAH, Section 4).    -- Originality and Quality --    To the best of my knowledge, the proposed NMBM algorithm is new. However, I feel that the derivation of this algorithm is relatively straightforward based on existing literature. Specifically, this algorithm is based on (1) Theorem 3 and (2) the linear program defined in equation 9. My understanding is that both Theorem 3 and the derivation of the linear program in equation 9 are relatively straightforward based on existing literature.    On the other hand, the experiment results in Section 4 are very strong and interesting. It is the main strength of this paper.    -- Clarity --    My understanding is that the writing of Section 3 and 4 can be (and should be) further polished.    Some key notations in the paper seem to be wrong:    (1) In Theorem 3, how can the value function v^\\pi(s) be in the convex hull of policies? Also, e_i is not a set.    (2) In equation 9, the linear program, \\eta should be another decision variable.     -- Pros and Cons --    Pros:    1) Strong experiments.    Cons:    1) Insufficient novelty for algorithm design.    2) No performance analysis for the proposed algorithm.    3) Clarity needs to be further improved.",
"Comment21": "Thanks very much for the review! For the algorithm, we agree that it is indeed straightforward. We would like to note, though, that dealing with the scalarized reward function has long been an open problem in inverse reinforcement learning. We have tried other (more complex) approaches but finally found out that the lower bound introduced in Theorem 3.1 is the most appropriate one. We believe that the estimation in Theorem 3.1 is a reasonable solution to the problem. On the other hand, there are rooms for other subtle methods related to distance measurement, in Section 3.2. We are working on employing that into the algorithm for better performance.    We thank the reviewer for the comments on the strong experiments. In fact, involving some real-world problem gives the implication beyond the typical simulator-based environments. It is important that the reviewer mentions *No performance analysis for the proposed algorithm*. Keeping that in mind, we are working on adding some results on openAI gym, which includes benchmarked tasks and quantitative evaluations.    We have corrected the notation typos in the updated draft and updated some other writings for its clarity.",
"Rating3": 4,
"Review3": "======== Summary ============    The authors consider a setup where there is a set of trajectories (s_t, a_t, r_t) where r_t is a *vector* of rewards. They assume that each agent is trying to maximize \\sum_t \\gamma^t (\\phi . r_t) where \\phi is a preference vector that lives on the simplex. Their goal is to calculate \\phi (and maybe also an optimal policy under \\phi?). The     The authors first prove that this problem can be decomposed into finding Q functions for optimal policies for each component of r_t individually, and then solving for \\phi that rationalizes the trajectory of actions in terms of these Q functions. Given the entire collection of trajectories, they perform off-policy Q-learning on each component of r_t in order to learn the Q function for that component, and then use linear programming to solve for \\phi based on these Q function.    ========== Comments =============    I think it's a worthwhile direction to combine IRL with modeling a diversity of preferences among agents. I can imagine several reasons you might want to do this, but the authors are not clear what their goal is besides *to propose methods that can help to understand the intricacy and complexity of human motivations and their behaviors*. Is the goal to do better policy prediction? To do better policy prediction conditional on \\phi? To infer \\phi to understand people's preferences from a social science perspective? These all seems reasonable but not sufficiently teased out in the work. (For comparison, IRL is typically - although not always - interested in learning the reward function in order to construct robust policies that maximize it). The authors also don't seem to solve a particular task of importance on the WoW dataset.    The theoretical approach seems sound, and I liked the way their algorithm was motivated and the way the problem was decomposed into off-policy Q-learning and then solving for \\phi.    However, I found myself quite confused in the experimental section (4.3). The authors evaluate their approach by action prediction. Given the trajectories, is \\phi computed for each player and then compute actions based on that value of \\phi? Is \\phi computed on the same trajectory data used for evaluation or a different subset? Or is action prediction performed in aggregate across the entire population? The experimental setup was never clarified for this (main) experiment.    I was also confused about the motivation for Figure 2 and Appendix D. The authors are showing that their predictions about which reward is motivating the players is consistent with external factors. But wouldn't you see the same thing if you just plotted the observed *rewards* themselves? E.g. players in a guild will achieve more Relationship reward.   The proposed approach takes the vector of reward, learns which actions are consistent with achieving each reward, then infers from the actions which reward is trying to be achieved. What advantages does this have vs. just looking at the empirical trajectory of rewards for each player/group?  I can certainly imagine that the IRL approach has certain advantages over looking at the empirical reward stream, but the authors have not talked about this nor compared against it experimentally.    The writing could also use some improvement for a future iteration, I've listed a few points below:    pg.1, Neither Brown & Sandholm nor Moravcik et al use *RL algorithms*  pg.1, Finn et al unmatched )  pg.1, *a scalar reward despite observed or not* -> *a scalar reward whether observed or not*  pg.2, *Either the range of* -> *Both the range of* (and this sentence needs further cleanup)  pg.2, *which records the pathing of players* ??  Theorem 3: *each of the set e_i has an unique element...* This isn't clear. I think you mean *For each e_i there is a unique vector v^\\pi(s) for all \\pi \\in \\Pi_{e_i} . The equality holds if these vectors are distinct for each e_i*.  pg. 5 *If otherwise all elements in \\phi are generative* how can they be negative if they are on the simplex?  pg.5 *we do not perform any scalarization on the reward...the model assumption is easier to be satisfied* I think this is a strange comparison to IRL because in IRL you're trying to find a (possibly parametric) function (s,a) -> R, whereas here you're *given* the vector R and are trying to find \\phi. So while you have more degrees of freedom by adding \\phi, you lose the original degrees of freedom in the reward function.",
"Comment31": "Thanks a lot for your detailed review! A quick note is that pg. 5 *If otherwise all elements in \\phi are generative* the \\phi is the existence specification by the separation theorem. It is not what mentioned in Section 3.2 and is not restricted to the simplex. We are updating Section 3.2 to avoid the possible ambiguity. We also note that the model is trained and tested on disjoint subsets of WoWAH. The spider map was calculated by the entire population though.    For the contribution of the paper, we believe it is indeed worth investigating combining IRL with the diversity of preferences among agents. In fact, the problem of IRL with scalar-valued reward has been long open. The assumption (that is used by almost every IRL algorithm) is too strong to complex agents (such as humans). We developed Theorem 3.1 which significantly weaker the assumption. We agree that armed with the theorem, Section 3.2 and Section 4 was not aiming at a clear objective as you may expect. To make the objective more clear, it is more reasonable to reproduce the policy (or the set of policy) that was used to generate the trajectory dataset. That is because we already have the reward vector and also the Pareto optimal relationship assumption, and the policy is the only unknown element. Some updates on the algorithm will be necessary then, which is currently undergoing.    For experiments, we agree that it can be confusing to demonstrate the real-world problem. There are several constraints to run the algorithm on the real-world dataset, such as the query of the state transition function. That makes the experiments itself more dependent on its context. As a solution, we find it better to add some well-known experiments such as openAI gym or gridworld with vector-reward, which provide a more intuitive understanding of the empirical performance of the algorithm. We are currently working on that.    We have updated the writing in the revised version of the paper. Thanks a lot for pointing them out!",
"Abstract": "In recent years, reinforcement learning methods have been applied to model gameplay with great success, achieving super-human performance in various environments, such as Atari, Go and Poker. However, those studies mostly focus on winning the game and have largely ignored the rich and complex human motivations, which are essential for understanding the agents' diverse behavior. In this paper, we present a multi-motivation behavior modeling which investigates the multifaceted human motivations and models the underlying value structure of the agents. Our approach extends inverse RL to the vectored-valued setting which imposes a much weaker assumption than previous studies. The vectorized rewards incorporate Pareto optimality, which is a powerful tool to explain a wide range of behavior by its optimality. For practical assessment, our algorithm is tested on the World of Warcraft Avatar History dataset spanning three years of the gameplay. Our experiments demonstrate the improvement over the scalarization-based methods on real-world problem settings.",
"Publication_Date": "2018/09/27",
"Readers": "everyone",
"URL": "https://openreview.net/pdf?id=SkVe3iA9Ym"
}
]